{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8757dcd9",
   "metadata": {},
   "source": [
    "# TP 2025-2026 — Intrusion & DDoS Detection using AI\n",
    "**Auteur :** TCHOMO KOMBOU THIERRY ARMEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed14bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder: C:\\Users\\thier\\Desktop\\TPs\\TP_1_5CS08_TP\\output\n"
     ]
    }
   ],
   "source": [
    "#Imports & dossiers\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, roc_auc_score, roc_curve, classification_report,\n",
    "                             precision_recall_curve, auc)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "OUT = Path('output')\n",
    "FIG = OUT / 'figures'\n",
    "OUT.mkdir(exist_ok=True)\n",
    "FIG.mkdir(exist_ok=True)\n",
    "print(\"Output folder:\", OUT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf263d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (488041, 86)\n",
      "Test shape:  (110399, 86)\n"
     ]
    }
   ],
   "source": [
    "#Chargement\n",
    "TRAIN = 'training_dataset_CIC_DDoS_2019.csv'\n",
    "TEST  = 'testing_dataset_CIC_DDoS_2019.csv'\n",
    "\n",
    "df_train = pd.read_csv(TRAIN)\n",
    "df_test  = pd.read_csv(TEST)\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test shape: \", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "281e8a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 488041 entries, 0 to 488040\n",
      "Data columns (total 86 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   Flow ID                      488041 non-null  object \n",
      " 1   Source IP                    488041 non-null  object \n",
      " 2   Source Port                  488041 non-null  int64  \n",
      " 3   Destination IP               488041 non-null  object \n",
      " 4   Destination Port             488041 non-null  int64  \n",
      " 5   Protocol                     488041 non-null  int64  \n",
      " 6   Timestamp                    488041 non-null  object \n",
      " 7   Flow Duration                488041 non-null  int64  \n",
      " 8   Total Fwd Packets            488041 non-null  int64  \n",
      " 9   Total Backward Packets       488041 non-null  int64  \n",
      " 10  Total Length of Fwd Packets  488041 non-null  float64\n",
      " 11  Total Length of Bwd Packets  488041 non-null  float64\n",
      " 12  Fwd Packet Length Max        488041 non-null  float64\n",
      " 13  Fwd Packet Length Min        488041 non-null  float64\n",
      " 14  Fwd Packet Length Mean       488041 non-null  float64\n",
      " 15  Fwd Packet Length Std        488041 non-null  float64\n",
      " 16  Bwd Packet Length Max        488041 non-null  float64\n",
      " 17  Bwd Packet Length Min        488041 non-null  float64\n",
      " 18  Bwd Packet Length Mean       488041 non-null  float64\n",
      " 19  Bwd Packet Length Std        488041 non-null  float64\n",
      " 20  Flow Bytes/s                 488041 non-null  float64\n",
      " 21  Flow Packets/s               488041 non-null  float64\n",
      " 22  Flow IAT Mean                488041 non-null  float64\n",
      " 23  Flow IAT Std                 488041 non-null  float64\n",
      " 24  Flow IAT Max                 488041 non-null  float64\n",
      " 25  Flow IAT Min                 488041 non-null  float64\n",
      " 26  Fwd IAT Total                488041 non-null  float64\n",
      " 27  Fwd IAT Mean                 488041 non-null  float64\n",
      " 28  Fwd IAT Std                  488041 non-null  float64\n",
      " 29  Fwd IAT Max                  488041 non-null  float64\n",
      " 30  Fwd IAT Min                  488041 non-null  float64\n",
      " 31  Bwd IAT Total                488041 non-null  float64\n",
      " 32  Bwd IAT Mean                 488041 non-null  float64\n",
      " 33  Bwd IAT Std                  488041 non-null  float64\n",
      " 34  Bwd IAT Max                  488041 non-null  float64\n",
      " 35  Bwd IAT Min                  488041 non-null  float64\n",
      " 36  Fwd PSH Flags                488041 non-null  int64  \n",
      " 37  Bwd PSH Flags                488041 non-null  int64  \n",
      " 38  Fwd URG Flags                488041 non-null  int64  \n",
      " 39  Bwd URG Flags                488041 non-null  int64  \n",
      " 40  Fwd Header Length            488041 non-null  int64  \n",
      " 41  Bwd Header Length            488041 non-null  int64  \n",
      " 42  Fwd Packets/s                488041 non-null  float64\n",
      " 43  Bwd Packets/s                488041 non-null  float64\n",
      " 44  Min Packet Length            488041 non-null  float64\n",
      " 45  Max Packet Length            488041 non-null  float64\n",
      " 46  Packet Length Mean           488041 non-null  float64\n",
      " 47  Packet Length Std            488041 non-null  float64\n",
      " 48  Packet Length Variance       488041 non-null  float64\n",
      " 49  FIN Flag Count               488041 non-null  int64  \n",
      " 50  SYN Flag Count               488041 non-null  int64  \n",
      " 51  RST Flag Count               488041 non-null  int64  \n",
      " 52  PSH Flag Count               488041 non-null  int64  \n",
      " 53  ACK Flag Count               488041 non-null  int64  \n",
      " 54  URG Flag Count               488041 non-null  int64  \n",
      " 55  CWE Flag Count               488041 non-null  int64  \n",
      " 56  ECE Flag Count               488041 non-null  int64  \n",
      " 57  Down/Up Ratio                488041 non-null  float64\n",
      " 58  Average Packet Size          488041 non-null  float64\n",
      " 59  Avg Fwd Segment Size         488041 non-null  float64\n",
      " 60  Avg Bwd Segment Size         488041 non-null  float64\n",
      " 61  Fwd Header Length.1          488041 non-null  int64  \n",
      " 62  Fwd Avg Bytes/Bulk           488041 non-null  int64  \n",
      " 63  Fwd Avg Packets/Bulk         488041 non-null  int64  \n",
      " 64  Fwd Avg Bulk Rate            488041 non-null  int64  \n",
      " 65  Bwd Avg Bytes/Bulk           488041 non-null  int64  \n",
      " 66  Bwd Avg Packets/Bulk         488041 non-null  int64  \n",
      " 67  Bwd Avg Bulk Rate            488041 non-null  int64  \n",
      " 68  Subflow Fwd Packets          488041 non-null  int64  \n",
      " 69  Subflow Fwd Bytes            488041 non-null  int64  \n",
      " 70  Subflow Bwd Packets          488041 non-null  int64  \n",
      " 71  Subflow Bwd Bytes            488041 non-null  int64  \n",
      " 72  Init_Win_bytes_forward       488041 non-null  int64  \n",
      " 73  Init_Win_bytes_backward      488041 non-null  int64  \n",
      " 74  act_data_pkt_fwd             488041 non-null  int64  \n",
      " 75  min_seg_size_forward         488041 non-null  int64  \n",
      " 76  Active Mean                  488041 non-null  float64\n",
      " 77  Active Std                   488041 non-null  float64\n",
      " 78  Active Max                   488041 non-null  float64\n",
      " 79  Active Min                   488041 non-null  float64\n",
      " 80  Idle Mean                    488041 non-null  float64\n",
      " 81  Idle Std                     488041 non-null  float64\n",
      " 82  Idle Max                     488041 non-null  float64\n",
      " 83  Idle Min                     488041 non-null  float64\n",
      " 84  Label                        488041 non-null  object \n",
      " 85  Target                       488041 non-null  object \n",
      "dtypes: float64(45), int64(35), object(6)\n",
      "memory usage: 320.2+ MB\n",
      "None\n",
      "\n",
      "Target counts (train):\n",
      "Target\n",
      "DDoS      440782\n",
      "Benign     47259\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label sample counts (train):\n",
      "Label\n",
      "Syn        326782\n",
      "BENIGN      47259\n",
      "LDAP        40871\n",
      "UDP         33083\n",
      "MSSQL       24436\n",
      "NetBIOS     15527\n",
      "UDPLag         83\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top missing (train):\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Constant columns: ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'FIN Flag Count', 'PSH Flag Count', 'ECE Flag Count', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate']\n",
      "\n",
      "Potential ID-like columns: ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Flow Duration', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min']\n",
      "\n",
      "Number of duplicate rows (train): 0\n",
      "\n",
      "Highly correlated pairs (>0.95): [('Flow Duration', 'Fwd IAT Total', 0.9994836444431536), ('Total Fwd Packets', 'Subflow Fwd Packets', 1.0), ('Total Backward Packets', 'Total Length of Bwd Packets', 0.9738630305756), ('Total Backward Packets', 'Subflow Bwd Packets', 1.0), ('Total Backward Packets', 'Subflow Bwd Bytes', 0.9738630305756), ('Total Length of Fwd Packets', 'Subflow Fwd Bytes', 1.0), ('Total Length of Bwd Packets', 'Subflow Bwd Packets', 0.9738630305756), ('Total Length of Bwd Packets', 'Subflow Bwd Bytes', 1.0), ('Fwd Packet Length Max', 'Fwd Packet Length Mean', 0.9618344123550492), ('Fwd Packet Length Max', 'Packet Length Mean', 0.9647568210759627), ('Fwd Packet Length Max', 'Average Packet Size', 0.9589977496817872), ('Fwd Packet Length Max', 'Avg Fwd Segment Size', 0.9618344123550492), ('Fwd Packet Length Min', 'Fwd Packet Length Mean', 0.9972748771905573), ('Fwd Packet Length Min', 'Min Packet Length', 0.9993201576606155), ('Fwd Packet Length Min', 'Packet Length Mean', 0.9920504453363511), ('Fwd Packet Length Min', 'Average Packet Size', 0.9956020810440536), ('Fwd Packet Length Min', 'Avg Fwd Segment Size', 0.9972748771905573), ('Fwd Packet Length Mean', 'Min Packet Length', 0.9965635898884232), ('Fwd Packet Length Mean', 'Packet Length Mean', 0.9952542299865905), ('Fwd Packet Length Mean', 'Average Packet Size', 0.9969057880186696), ('Fwd Packet Length Mean', 'Avg Fwd Segment Size', 1.0), ('Bwd Packet Length Max', 'Bwd Packet Length Std', 0.9685921979962007), ('Bwd Packet Length Mean', 'Avg Bwd Segment Size', 1.0), ('Flow Packets/s', 'Fwd Packets/s', 0.9994617283551227), ('Flow IAT Mean', 'Flow IAT Std', 0.9502819226104048), ('Flow IAT Mean', 'Fwd IAT Mean', 0.973633476833233), ('Flow IAT Std', 'Fwd IAT Mean', 0.9666417294765952), ('Flow IAT Std', 'Fwd IAT Std', 0.991196276217478), ('Flow IAT Max', 'Fwd IAT Max', 0.9982563494309504), ('Flow IAT Max', 'Idle Mean', 0.9748221842545366)]\n",
      "Saved corr_heatmap_subset.png\n",
      "Saved target_distribution.png\n"
     ]
    }
   ],
   "source": [
    "#EDA complet\n",
    "# 1) shape / dtypes\n",
    "print(df_train.info())\n",
    "\n",
    "# 2) Target / Label distributions\n",
    "print(\"\\nTarget counts (train):\")\n",
    "print(df_train['Target'].value_counts(dropna=False))\n",
    "print(\"\\nLabel sample counts (train):\")\n",
    "print(df_train['Label'].value_counts().head(20))\n",
    "\n",
    "# 3) Missing values\n",
    "missing = df_train.isnull().sum().sort_values(ascending=False)\n",
    "print(\"\\nTop missing (train):\")\n",
    "print(missing[missing>0].head(30))\n",
    "missing.to_csv(OUT/'missing_counts_train.csv')\n",
    "\n",
    "# 4) Constant columns\n",
    "const_cols = [c for c in df_train.columns if df_train[c].nunique()==1]\n",
    "print(\"\\nConstant columns:\", const_cols)\n",
    "\n",
    "# 5) ID-like columns\n",
    "id_like = [c for c in df_train.columns if any(k in c.lower() for k in ['flow','ip','timestamp','id','src','dst'])]\n",
    "print(\"\\nPotential ID-like columns:\", id_like)\n",
    "\n",
    "# 6) Duplicates\n",
    "print(\"\\nNumber of duplicate rows (train):\", df_train.duplicated().sum())\n",
    "\n",
    "# 7) Numeric correlation & highly correlated pairs\n",
    "num = df_train.select_dtypes(include=['int64','float64']).apply(pd.to_numeric, errors='coerce')\n",
    "cor = num.corr().abs()\n",
    "pairs = []\n",
    "for i in range(len(cor.columns)):\n",
    "    for j in range(i+1, len(cor.columns)):\n",
    "        v = cor.iloc[i,j]\n",
    "        if pd.notna(v) and v > 0.95:\n",
    "            pairs.append((cor.columns[i], cor.columns[j], v))\n",
    "print(\"\\nHighly correlated pairs (>0.95):\", pairs[:30])\n",
    "\n",
    "# 8) Save correlation heatmap (subset)\n",
    "cols = num.columns[:30]\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(num[cols].corr(), cmap='viridis')\n",
    "plt.title('Correlation heatmap (first 30 numeric features)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG/'corr_heatmap_subset.png', dpi=200)\n",
    "plt.close()\n",
    "print(\"Saved corr_heatmap_subset.png\")\n",
    "\n",
    "# 9) Target distribution plot\n",
    "plt.figure()\n",
    "df_train['Target'].value_counts().plot(kind='bar')\n",
    "plt.title('Target distribution (train)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG/'target_distribution.png', dpi=200)\n",
    "plt.close()\n",
    "print(\"Saved target_distribution.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfae6388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONST_COLS : ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'FIN Flag Count', 'PSH Flag Count', 'ECE Flag Count', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate']\n",
      "ID_COLS    : ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Flow Duration', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min']\n",
      "Nb de paires corrélées (>0.95) : 50\n",
      "Nombre de colonnes à supprimer pour corrélation : 27\n",
      "Exemples de colonnes supprimées (corr élevées) : ['Fwd IAT Min', 'Subflow Bwd Bytes', 'Avg Bwd Segment Size', 'Fwd IAT Mean', 'Fwd Header Length.1', 'Average Packet Size', 'Fwd Packet Length Mean', 'Min Packet Length', 'Idle Min', 'Bwd IAT Std', 'Subflow Bwd Packets', 'RST Flag Count', 'Flow IAT Std', 'Bwd Packet Length Std', 'Idle Max']\n",
      "Shape base train (après drop ID + constantes): (488041, 54)\n",
      "Shape base test  (après drop ID + constantes): (110399, 54)\n",
      "Shapes après filtrage corrélations :\n",
      "  X_rf  : (488041, 54)\n",
      "  X_knn : (488041, 35)\n",
      "  X_ada : (488041, 35)\n",
      "Train/val RF  : (341628, 54) (146413, 54)\n",
      "Train/val KNN : (341628, 35) (146413, 35)\n",
      "Train/val Ada : (341628, 35) (146413, 35)\n",
      "Dimension KNN avant PCA : 35\n",
      "Dimension KNN après PCA : 21\n"
     ]
    }
   ],
   "source": [
    "#EDA\n",
    "\n",
    "CONST_COLS = const_cols          # colonnes constantes déjà trouvées\n",
    "ID_COLS    = id_like             # colonnes identifiantes déjà trouvées\n",
    "HIGH_CORR_PAIRS = pairs          # liste de tuples (col1, col2, corr)\n",
    "\n",
    "print(\"CONST_COLS :\", CONST_COLS)\n",
    "print(\"ID_COLS    :\", ID_COLS)\n",
    "print(\"Nb de paires corrélées (>0.95) :\", len(HIGH_CORR_PAIRS))\n",
    "\n",
    "# Pour chaque paire (a, b, corr), on garde a et on supprime b\n",
    "to_drop_corr = set()\n",
    "for a, b, v in HIGH_CORR_PAIRS:\n",
    "    # on décide de toujours supprimer le deuxième de la paire\n",
    "    to_drop_corr.add(b)\n",
    "\n",
    "TO_DROP_CORR = list(to_drop_corr)\n",
    "print(\"Nombre de colonnes à supprimer pour corrélation :\", len(TO_DROP_CORR))\n",
    "print(\"Exemples de colonnes supprimées (corr élevées) :\", TO_DROP_CORR[:15])\n",
    "\n",
    "# 2) Fonction qui nettoie : Target -> binaire, drop ID + constantes, encodage Protocol, conversion numérique\n",
    "def clean_base_from_eda(df):\n",
    "    df2 = df.copy()\n",
    "    # Cible binaire\n",
    "    df2['Target'] = df2['Target'].astype(str).map({'DDoS': 1, 'Benign': 0}).fillna(0).astype(int)\n",
    "    \n",
    "    # Drop identifiants + colonnes constantes\n",
    "    df2 = df2.drop(columns=ID_COLS + CONST_COLS, errors='ignore')\n",
    "    \n",
    "    y = df2['Target'].astype(int)\n",
    "    X = df2.drop(columns=['Label', 'Target'], errors=True)\n",
    "    \n",
    "    # Encodage de Protocol si présent\n",
    "    if 'Protocol' in X.columns:\n",
    "        X['Protocol'] = X['Protocol'].astype(str)\n",
    "        X = pd.get_dummies(X, columns=['Protocol'], drop_first=True)\n",
    "    \n",
    "    # Tout en numérique + imputation simple\n",
    "    X = X.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    return X, y\n",
    "\n",
    "# Application à train / test\n",
    "X_base, y_full = clean_base_from_eda(df_train)\n",
    "X_test_base, y_test_full = clean_base_from_eda(df_test)\n",
    "\n",
    "print(\"Shape base train (après drop ID + constantes):\", X_base.shape)\n",
    "print(\"Shape base test  (après drop ID + constantes):\", X_test_base.shape)\n",
    "\n",
    "# 3) Construction des jeux de features par modèle\n",
    "\n",
    "# RF : on garde toutes les colonnes restantes (corrélées incluses)\n",
    "X_rf = X_base.copy()\n",
    "\n",
    "# KNN : on supprime toutes les colonnes fortement corrélées\n",
    "X_knn = X_base.drop(columns=TO_DROP_CORR, errors='ignore')\n",
    "\n",
    "# AdaBoost : même logique que KNN (suppression corrélées), sans PCA ensuite\n",
    "X_ada = X_base.drop(columns=TO_DROP_CORR, errors='ignore')\n",
    "\n",
    "print(\"Shapes après filtrage corrélations :\")\n",
    "print(\"  X_rf  :\", X_rf.shape)\n",
    "print(\"  X_knn :\", X_knn.shape)\n",
    "print(\"  X_ada :\", X_ada.shape)\n",
    "\n",
    "# 4) Split train / validation (même y, même random_state pour cohérence)\n",
    "X_rf_train, X_rf_val, y_rf_train, y_rf_val = train_test_split(\n",
    "    X_rf, y_full, test_size=0.3, random_state=42, shuffle=True, stratify=y_full\n",
    ")\n",
    "X_knn_train, X_knn_val, y_knn_train, y_knn_val = train_test_split(\n",
    "    X_knn, y_full, test_size=0.3, random_state=42, shuffle=True, stratify=y_full\n",
    ")\n",
    "X_ada_train, X_ada_val, y_ada_train, y_ada_val = train_test_split(\n",
    "    X_ada, y_full, test_size=0.3, random_state=42, shuffle=True, stratify=y_full\n",
    ")\n",
    "\n",
    "# Adapter X_test pour chaque jeu de colonnes\n",
    "X_rf_test  = X_test_base[X_rf.columns]\n",
    "X_knn_test = X_test_base[X_knn.columns]\n",
    "X_ada_test = X_test_base[X_ada.columns]\n",
    "\n",
    "print(\"Train/val RF  :\", X_rf_train.shape, X_rf_val.shape)\n",
    "print(\"Train/val KNN :\", X_knn_train.shape, X_knn_val.shape)\n",
    "print(\"Train/val Ada :\", X_ada_train.shape, X_ada_val.shape)\n",
    "\n",
    "# 5) StandardScaler pour les trois modèles\n",
    "\n",
    "scaler_rf  = StandardScaler()\n",
    "scaler_knn = StandardScaler()\n",
    "scaler_ada = StandardScaler()\n",
    "\n",
    "X_rf_train_scaled  = scaler_rf.fit_transform(X_rf_train)\n",
    "X_rf_val_scaled    = scaler_rf.transform(X_rf_val)\n",
    "X_rf_test_scaled   = scaler_rf.transform(X_rf_test)\n",
    "\n",
    "X_knn_train_scaled = scaler_knn.fit_transform(X_knn_train)\n",
    "X_knn_val_scaled   = scaler_knn.transform(X_knn_val)\n",
    "X_knn_test_scaled  = scaler_knn.transform(X_knn_test)\n",
    "\n",
    "X_ada_train_scaled = scaler_ada.fit_transform(X_ada_train)\n",
    "X_ada_val_scaled   = scaler_ada.transform(X_ada_val)\n",
    "X_ada_test_scaled  = scaler_ada.transform(X_ada_test)\n",
    "\n",
    "# 6) PCA pour KNN uniquement (on garde ~95% de la variance)\n",
    "pca_knn = PCA(n_components=0.95)\n",
    "X_knn_train_pca = pca_knn.fit_transform(X_knn_train_scaled)\n",
    "X_knn_val_pca   = pca_knn.transform(X_knn_val_scaled)\n",
    "X_knn_test_pca  = pca_knn.transform(X_knn_test_scaled)\n",
    "\n",
    "print(\"Dimension KNN avant PCA :\", X_knn_train.shape[1])\n",
    "print(\"Dimension KNN après PCA :\", X_knn_train_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d2126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RANDOM FOREST ===\n",
      "Classification report (test) - RandomForest :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3253    0.9949    0.4903     11457\n",
      "           1     0.9992    0.7611    0.8641     98942\n",
      "\n",
      "    accuracy                         0.7854    110399\n",
      "   macro avg     0.6623    0.8780    0.6772    110399\n",
      "weighted avg     0.9293    0.7854    0.8253    110399\n",
      "\n",
      "\n",
      "=== KNN (avec PCA) ===\n"
     ]
    }
   ],
   "source": [
    "# === PARTIE C : Entraînement et évaluation des modèles (RF, KNN, AdaBoost) ===\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Sécurité : s'assurer que les dossiers existent\n",
    "OUT.mkdir(exist_ok=True)\n",
    "FIG.mkdir(exist_ok=True)\n",
    "\n",
    "val_results = []\n",
    "test_results = []\n",
    "\n",
    "# ========= 1) RANDOM FOREST =========\n",
    "print(\"\\n=== RANDOM FOREST ===\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf.fit(X_rf_train_scaled, y_rf_train)\n",
    "\n",
    "# Validation\n",
    "y_rf_val_pred  = rf.predict(X_rf_val_scaled)\n",
    "y_rf_val_proba = rf.predict_proba(X_rf_val_scaled)[:, 1]\n",
    "\n",
    "rf_val_acc  = accuracy_score(y_rf_val, y_rf_val_pred)\n",
    "rf_val_prec = precision_score(y_rf_val, y_rf_val_pred, zero_division=0)\n",
    "rf_val_rec  = recall_score(y_rf_val, y_rf_val_pred, zero_division=0)\n",
    "rf_val_f1   = f1_score(y_rf_val, y_rf_val_pred, zero_division=0)\n",
    "rf_val_auc  = roc_auc_score(y_rf_val, y_rf_val_proba)\n",
    "\n",
    "cm_rf_val = confusion_matrix(y_rf_val, y_rf_val_pred)\n",
    "\n",
    "# Confusion matrix (val)\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(cm_rf_val, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion matrix - RandomForest (val)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / 'confusion_RandomForest_val.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# ROC (val)\n",
    "fpr, tpr, _ = roc_curve(y_rf_val, y_rf_val_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC={rf_val_auc:.4f}')\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.title('ROC - RandomForest (val)')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / 'roc_RandomForest_val.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "val_results.append({\n",
    "    'model': 'RandomForest',\n",
    "    'accuracy': rf_val_acc,\n",
    "    'precision': rf_val_prec,\n",
    "    'recall': rf_val_rec,\n",
    "    'f1': rf_val_f1,\n",
    "    'roc_auc': rf_val_auc\n",
    "})\n",
    "\n",
    "# Test\n",
    "# Probabilités prédites pour la classe 1\n",
    "y_rf_test_proba = rf.predict_proba(X_rf_test_scaled)[:, 1]\n",
    "\n",
    "# Seuil personnalisé, par exemple 0.5\n",
    "threshold = 0.5\n",
    "y_rf_test_pred = (y_rf_test_proba >= threshold).astype(int)\n",
    "\n",
    "y_rf_test_proba = rf.predict_proba(X_rf_test_scaled)[:, 1]\n",
    "\n",
    "rf_test_acc  = accuracy_score(y_test_full, y_rf_test_pred)\n",
    "rf_test_prec = precision_score(y_test_full, y_rf_test_pred, zero_division=0)\n",
    "rf_test_rec  = recall_score(y_test_full, y_rf_test_pred, zero_division=0)\n",
    "rf_test_f1   = f1_score(y_test_full, y_rf_test_pred, zero_division=0)\n",
    "rf_test_auc  = roc_auc_score(y_test_full, y_rf_test_proba)\n",
    "\n",
    "cm_rf_test = confusion_matrix(y_test_full, y_rf_test_pred)\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(cm_rf_test, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion matrix - RandomForest (test)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / 'confusion_RandomForest_test.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_full, y_rf_test_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC={rf_test_auc:.4f}')\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.title('ROC - RandomForest (test)')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / 'roc_RandomForest_test.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "test_results.append({\n",
    "    'model': 'RandomForest',\n",
    "    'accuracy': rf_test_acc,\n",
    "    'precision': rf_test_prec,\n",
    "    'recall': rf_test_rec,\n",
    "    'f1': rf_test_f1,\n",
    "    'roc_auc': rf_test_auc\n",
    "})\n",
    "\n",
    "print(\"Classification report (test) - RandomForest :\")\n",
    "print(classification_report(y_test_full, y_rf_test_pred, digits=4))\n",
    "\n",
    "\n",
    "# ========= 2) KNN (avec PCA) =========\n",
    "print(\"\\n=== KNN (avec PCA) ===\")\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_knn_train_pca, y_knn_train)\n",
    "\n",
    "# Validation\n",
    "y_knn_val_pred  = knn.predict(X_knn_val_pca)\n",
    "y_knn_val_proba = knn.predict_proba(X_knn_val_pca)[:, 1]\n",
    "\n",
    "knn_val_acc  = accuracy_score(y_knn_val, y_knn_val_pred)\n",
    "knn_val_prec = precision_score(y_knn_val, y_knn_val_pred, zero_division=0)\n",
    "knn_val_rec  = recall_score(y_knn_val, y_knn_val_pred, zero_division=0)\n",
    "knn_val_f1   = f1_score(y_knn_val, y_knn_val_pred, zero_division=0)\n",
    "knn_val_auc  = roc_auc_score(y_knn_val, y_knn_val_proba)\n",
    "\n",
    "cm_knn_val = confusion_matrix(y_knn_val, y_knn_val_pred)\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(cm_knn_val, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion matrix - KNN (val)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / 'confusion_KNN_val.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_knn_val, y_knn_val_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC={knn_val_auc:.4f}')\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.title('ROC - KNN (val)')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / 'roc_KNN_val.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "val_results.append({\n",
    "    'model': 'KNN',\n",
    "    'accuracy': knn_val_acc,\n",
    "    'precision': knn_val_prec,\n",
    "    'recall': knn_val_rec,\n",
    "    'f1': knn_val_f1,\n",
    "    'roc_auc': knn_val_auc\n",
    "})\n",
    "\n",
    "# Test\n",
    "y_knn_test_pred  = knn.predict(X_knn_test_pca)\n",
    "y_knn_test_proba = knn.predict_proba(X_knn_test_pca)[:, 1]\n",
    "\n",
    "knn_test_acc  = accuracy_score(y_test_full, y_knn_test_pred)\n",
    "knn_test_prec = precision_score(y_test_full, y_knn_test_pred, zero_division=0)\n",
    "knn_test_rec  = recall_score(y_test_full, y_knn_test_pred, zero_division=0)\n",
    "knn_test_f1   = f1_score(y_test_full, y_knn_test_pred, zero_division=0)\n",
    "knn_test_auc  = roc_auc_score(y_test_full, y_knn_test_proba)\n",
    "\n",
    "cm_knn_test = confusion_matrix(y_test_full, y_knn_test_pred)\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(cm_knn_test, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion matrix - KNN (test)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / 'confusion_KNN_test.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_full, y_knn_test_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC={knn_test_auc:.4f}')\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.title('ROC - KNN (test)')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / 'roc_KNN_test.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "test_results.append({\n",
    "    'model': 'KNN',\n",
    "    'accuracy': knn_test_acc,\n",
    "    'precision': knn_test_prec,\n",
    "    'recall': knn_test_rec,\n",
    "    'f1': knn_test_f1,\n",
    "    'roc_auc': knn_test_auc\n",
    "})\n",
    "\n",
    "print(\"Classification report (test) - KNN :\")\n",
    "print(classification_report(y_test_full, y_knn_test_pred, digits=4))\n",
    "\n",
    "\n",
    "# ========= 3) ADABOOST =========\n",
    "print(\"\\n=== AdaBoost ===\")\n",
    "ada = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ada.fit(X_ada_train_scaled, y_ada_train)\n",
    "\n",
    "# Validation\n",
    "y_ada_val_pred  = ada.predict(X_ada_val_scaled)\n",
    "y_ada_val_proba = ada.predict_proba(X_ada_val_scaled)[:, 1]\n",
    "\n",
    "ada_val_acc  = accuracy_score(y_ada_val, y_ada_val_pred)\n",
    "ada_val_prec = precision_score(y_ada_val, y_ada_val_pred, zero_division=0)\n",
    "ada_val_rec  = recall_score(y_ada_val, y_ada_val_pred, zero_division=0)\n",
    "ada_val_f1   = f1_score(y_ada_val, y_ada_val_pred, zero_division=0)\n",
    "ada_val_auc  = roc_auc_score(y_ada_val, y_ada_val_proba)\n",
    "\n",
    "cm_ada_val = confusion_matrix(y_ada_val, y_ada_val_pred)\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(cm_ada_val, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion matrix - AdaBoost (val)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / 'confusion_AdaBoost_val.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_ada_val, y_ada_val_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC={ada_val_auc:.4f}')\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.title('ROC - AdaBoost (val)')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / 'roc_AdaBoost_val.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "val_results.append({\n",
    "    'model': 'AdaBoost',\n",
    "    'accuracy': ada_val_acc,\n",
    "    'precision': ada_val_prec,\n",
    "    'recall': ada_val_rec,\n",
    "    'f1': ada_val_f1,\n",
    "    'roc_auc': ada_val_auc\n",
    "})\n",
    "\n",
    "# Test\n",
    "y_ada_test_pred  = ada.predict(X_ada_test_scaled)\n",
    "y_ada_test_proba = ada.predict_proba(X_ada_test_scaled)[:, 1]\n",
    "\n",
    "ada_test_acc  = accuracy_score(y_test_full, y_ada_test_pred)\n",
    "ada_test_prec = precision_score(y_test_full, y_ada_test_pred, zero_division=0)\n",
    "ada_test_rec  = recall_score(y_test_full, y_ada_test_pred, zero_division=0)\n",
    "ada_test_f1   = f1_score(y_test_full, y_ada_test_pred, zero_division=0)\n",
    "ada_test_auc  = roc_auc_score(y_test_full, y_ada_test_proba)\n",
    "\n",
    "cm_ada_test = confusion_matrix(y_test_full, y_ada_test_pred)\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(cm_ada_test, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion matrix - AdaBoost (test)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / 'confusion_AdaBoost_test.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_full, y_ada_test_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC={ada_test_auc:.4f}')\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.title('ROC - AdaBoost (test)')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / 'roc_AdaBoost_test.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "test_results.append({\n",
    "    'model': 'AdaBoost',\n",
    "    'accuracy': ada_test_acc,\n",
    "    'precision': ada_test_prec,\n",
    "    'recall': ada_test_rec,\n",
    "    'f1': ada_test_f1,\n",
    "    'roc_auc': ada_test_auc\n",
    "})\n",
    "\n",
    "print(\"Classification report (test) - AdaBoost :\")\n",
    "print(classification_report(y_test_full, y_ada_test_pred, digits=4))\n",
    "\n",
    "\n",
    "# ========= 4) Sauvegarde des résultats globaux =========\n",
    "val_df  = pd.DataFrame(val_results)\n",
    "test_df = pd.DataFrame(test_results)\n",
    "\n",
    "print(\"\\n=== Résultats validation ===\")\n",
    "print(val_df)\n",
    "\n",
    "print(\"\\n=== Résultats test ===\")\n",
    "print(test_df)\n",
    "\n",
    "val_df.to_csv(OUT / 'val_metrics.csv', index=False)\n",
    "test_df.to_csv(OUT / 'test_metrics.csv', index=False)\n",
    "\n",
    "with open(OUT / 'val_metrics.json', 'w') as f:\n",
    "    json.dump(val_results, f, indent=2)\n",
    "with open(OUT / 'test_metrics.json', 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "print(\"\\nMetrics and figures saved in:\", OUT, \"and\", FIG)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
